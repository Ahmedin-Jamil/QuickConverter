# QC: Financial Data Connector (Enterprise ETL)

**Interview-Grade System for Bank Statement Ingestion & Normalization**

QC is a specialized **Financial ETL Connector** designed to transform heterogeneous bank statement formats into standardized schemas consumable by downstream accounting systems. This project demonstrates professional Data Engineering patterns suitable for Accenture-style ETL & Connector roles.

---

## ğŸ¯ Core Capabilities

### 1. Transaction Categorization (Rule-Based)
- Deterministic keyword-based categorization engine
- 10 predefined categories: `Transport`, `Meals`, `Utilities`, `Subscriptions`, `Transfers`, `ATM/Cash`, `Income`, `Shopping`, `Healthcare`, `Fees`
- Fully transparent rules in `categorize.py` â€” no black-box AI
- Configurable and extensible for enterprise customization

### 2. Reconciliation Check
- Validates: `Opening Balance + Credits âˆ’ Debits = Closing Balance`
- Flags mismatches with delta amount in Excel output
- Visual status indicator in UI: âœ… Balanced / âš ï¸ Mismatch

### 3. Data Quality Report
- DQ flags: `CLEAN` (table-extracted), `RECOVERED` (heuristic), `SUSPECT` (true anomaly), `NON_TRANSACTION` (metadata)
- Transaction eligibility filtering separates actual transactions from balance/summary rows
- Dedicated Excel sheet showing:
  - Summary statistics (clean/recovered/suspect/non-transaction counts)
  - Flagged rows table with reasons (duplicate, imbalance, format issue)
- Human-readable, audit-friendly output

---

## ğŸ—ï¸ ETL Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EXTRACT   â”‚â”€â”€â”€â–¶â”‚   FILTER    â”‚â”€â”€â”€â–¶â”‚  TRANSFORM  â”‚â”€â”€â”€â–¶â”‚     DQ      â”‚â”€â”€â”€â–¶â”‚    LOAD     â”‚
â”‚ pdfplumber  â”‚    â”‚ Eligibility â”‚    â”‚ Categorize  â”‚    â”‚ Validate    â”‚    â”‚ Multi-sheet â”‚
â”‚ CSV Parser  â”‚    â”‚ Separation  â”‚    â”‚ Normalize   â”‚    â”‚ Reconcile   â”‚    â”‚   Excel     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Design Principles

| Principle | Implementation |
|-----------|----------------|
| **Deterministic** | Same input â†’ Same output. Zero randomness. |
| **Auditable** | Every extraction rule traceable in code. |
| **No AI Dependency** | Runs without LLM/API calls. Sub-second latency. |
| **Schema-First** | Strict `TypedDict` schemas for data integrity. |
| **API-First** | All processing via REST endpoints. UI-independent. |

---

## ğŸ“‚ Project Structure

```
backend/
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py      # PDF/CSV Hybrid Parsers
â”‚   â”œâ”€â”€ filter.py       # Transaction Eligibility Filtering
â”‚   â”œâ”€â”€ transform.py    # Regex Normalization & Dedup
â”‚   â”œâ”€â”€ categorize.py   # Rule-Based Transaction Categorization
â”‚   â”œâ”€â”€ dq.py           # Data Quality Engine + Reconciliation
â”‚   â”œâ”€â”€ load.py         # Multi-sheet Excel Writer
â”‚   â”œâ”€â”€ pipeline.py     # Orchestrator
â”‚   â””â”€â”€ schema.py       # Strict Schema Definitions
â”œâ”€â”€ app.py              # Flask API
â””â”€â”€ supabase_client.py  # Observability Layer

src/
â””â”€â”€ main.js             # Frontend Logic

index.html              # Finance-First UI
style.css               # Professional Styling
```

---

## ğŸ“Š Excel Output Structure

| Sheet | Contents |
|-------|----------|
| **Transactions** | Date, Description, Category, Debit, Credit, Balance, DQ Flag |
| **Financial Summary** | Opening/Closing Balance, Totals, Net Change, Reconciliation Check |
| **Data Quality Report** | Summary stats, Flagged rows table with reasons |

---

## ğŸ“ˆ Enterprise Scalability

This Python MVP maps directly to enterprise patterns:

| Component | MVP (Python) | Enterprise (Spark/Java) |
|-----------|--------------|-------------------------|
| **Compute** | Flask / Single Node | Apache Spark / AWS Lambda |
| **Parsing** | `pdfplumber` | Apache Tika / AWS Textract |
| **Storage** | Local / Outputs | S3 Data Lake / Delta Lake |
| **Logging** | Supabase | Kafka + ELK Stack |
| **Categorization** | Python Dict Rules | External Config / Rules Engine |

---

## ğŸš¦ Getting Started

```bash
# Backend
pip install -r requirements.txt
python backend/app.py

# Frontend
npm install
npm run dev
```

Open `http://localhost:5173` and upload a bank statement PDF.

---

## ğŸ“ Interview Talking Points

1. **Why Deterministic over AI?**
   - Reproducibility for financial auditing
   - Zero API costs per transaction
   - Sub-second latency vs 10-30s for LLM reasoning
   - Fully traceable extraction rules

2. **Why separate Transactions from Metadata?**
   - Bank statements mix actual transactions with summary rows (opening/closing balance)
   - Counting balance rows as transactions breaks reconciliation
   - QC intentionally filters them using keyword rules (`filter.py`)
   - Result: Transactions sheet contains only debits/credits with financial impact

3. **How does Reconciliation work?**
   - Extracts Opening/Closing balance from metadata rows
   - Sums debits/credits from eligible transactions only
   - Validates: `Opening + Credits - Debits = Closing`
   - Attribution on failure: "Missing rows" vs "Rounding/fees"

4. **What makes DQ "audit-friendly"?**
   - Row-level flagging with specific reasons
   - Separate sheet for flagged records
   - Four-tier classification: CLEAN â†’ RECOVERED â†’ SUSPECT â†’ NON_TRANSACTION

5. **How would you scale this?**
   - Stateless pipeline â†’ horizontal scaling behind load balancer
   - Replace pdfplumber with distributed parsers (Tika, Textract)
   - Category rules â†’ external config service for runtime updates

---

Built for demonstrating **Data Engineering** and **ETL Connector** expertise.

